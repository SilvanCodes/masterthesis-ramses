configfile: "config.yaml"
conda: "envs/global.yaml"


localrules:
    download_reference,
    download_annotation,
    download_utr_script,
    add_utr_to_annotation,
    all,

# set to WANDB_MODE=disabled when not using it
envvars: "WANDB_MODE"


import pandas as pd
from Bio import SeqIO
import gzip
import bioframe as bf
from gpn.data import load_table, Genome, filter_length, make_windows
import more_itertools


WINDOW_SIZE = config["window_size"]
EMBEDDING_WINDOW_SIZE = 100


models = [
    "gonzalobenegas/gpn-brassicales",
]


rule all:
    input:
        expand("output/embedding/umap/{model}.parquet", model=models),


rule download_reference:
    output:
        "output/genome.fa.gz",
    shell:
        "wget --no-check-certificate {config[FASTA_URL]} -O {output}"


rule download_annotation:
    output:
        "output/annotation.gtf.gz",
    shell:
        "wget --no-check-certificate {config[GTF_URL]} -O {output}"

rule download_utr_script:
    params:
        url = "https://ftp.ncbi.nlm.nih.gov/genomes/TOOLS/add_utrs_to_gff/add_utrs_to_gff.py"
    output:
        "output/add_utrs_to_gff.py",
    shell:
        "wget --no-check-certificate {params.url} -O {output}"

rule add_utr_to_annotation:
    input:
        "output/add_utrs_to_gff.py",
        "output/annotation.gtf.gz",
    output:
        "output/annotation_utr.gtf",
    shell:
        "python {input} > {output}"


rule mask_repeats:
    input:
        "output/genome.fa.gz",
    output:
        "output/mask_repeats/genome.fa.out.gff",
    conda:
        "envs/repeatmasker.yaml"
    threads: workflow.cores
    shell:
        "RepeatMasker -species Tribolium_castaneum -dir output/mask_repeats -pa {threads} -gff {input}"


rule expand_annotation:
    input:
        "output/annotation_utr.gtf",
        # "output/mask_repeats/genome.fa.out.gff",
    output:
        "output/annotation.expanded.parquet",
    conda:
        "envs/expand_annotation.yaml"
    script:
        "scripts/expand_annotation.py"


rule define_embedding_windows:
    input:
        "output/annotation.expanded.parquet",
        "output/genome.fa.gz",
    output:
        "output/embedding/windows.parquet",
    run:
        gtf = pd.read_parquet(input[0])
        genome = Genome(input[1])
        # genome.filter_chroms(["1", "2", "3", "4", "5"])
        defined_intervals = genome.get_defined_intervals()
        defined_intervals = filter_length(defined_intervals, WINDOW_SIZE)
        windows = make_windows(defined_intervals, WINDOW_SIZE, EMBEDDING_WINDOW_SIZE)
        windows.rename(columns={"start": "full_start", "end": "full_end"}, inplace=True)

        windows["start"] = (
            windows.full_start + windows.full_end
        ) // 2 - EMBEDDING_WINDOW_SIZE // 2
        windows["end"] = windows.start + EMBEDDING_WINDOW_SIZE

        features_of_interest = [
            "intergenic",
            "CDS",
            "intron",
            # "three_prime_UTR",
            # "five_prime_UTR",
            # "ncRNA_gene",
            # "Repeat",
        ]

        for f in features_of_interest:
            print(f)
            windows = bf.coverage(windows, gtf[gtf.feature == f])
            windows.rename(columns=dict(coverage=f), inplace=True)

            # we keep if the center 100 bp are exactly covered by just on of the region of interest
        windows = windows[
            (windows[features_of_interest] == EMBEDDING_WINDOW_SIZE).sum(axis=1) == 1
        ]
        windows["Region"] = windows[features_of_interest].idxmax(axis=1)
        windows.drop(columns=features_of_interest, inplace=True)

        windows.rename(
            columns={"start": "center_start", "end": "center_end"}, inplace=True
        )
        windows.rename(columns={"full_start": "start", "full_end": "end"}, inplace=True)
        print(windows)
        windows.to_parquet(output[0], index=False)


rule get_embeddings:
    input:
        "output/embedding/windows.parquet",
        "output/genome.fa.gz",
    output:
        "output/embedding/embeddings/{model}.parquet",
    conda:
        "envs/gpn.yaml"
    threads: workflow.cores
    resources:
        slurm_extra="-G 2"
    shell:
        """
        python -m gpn.ss.get_embeddings {input} {EMBEDDING_WINDOW_SIZE} \
        {wildcards.model} {output} --per_device_batch_size 4000 --is_file \
        --dataloader_num_workers {threads}
        """

rule run_umap:
    input:
        "{anything}/embeddings/{model}.parquet",
    output:
        "{anything}/umap/{model}.parquet",
    conda:
        "envs/umap.yaml"
    script:
        "scripts/umap.py"
    # run:
    #     from umap import UMAP

    #     embeddings = pd.read_parquet(input[0])
    #     proj = Pipeline(
    #         [
    #             ("scaler", StandardScaler()),
    #             ("umap", UMAP(random_state=42, verbose=True)),
    #         ]
    #     ).fit_transform(embeddings)
    #     proj = pd.DataFrame(proj, columns=["UMAP1", "UMAP2"])
    #     proj.to_parquet(output[0], index=False)