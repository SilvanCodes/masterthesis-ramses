{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390abdbc-898a-46aa-9bc9-29e089551b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "file_path = \"nucleotide_dependency_maps/output/download_fasta/GCF_000001735.4.fna.gz\"\n",
    "\n",
    "records = list(SeqIO.parse(\"file.fasta\", \"fasta\"))\n",
    "for record in records:\n",
    "    print(record.id, len(record.seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae63f149-ed7f-4457-8bae-6d33543bb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4aaca3-4c7d-4a84-add5-b20571d523d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [2., 2., 2.],\n",
       "         [3., 3., 3.],\n",
       "         [4., 4., 4.]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input = torch.tensor([[[1.0,1.0,1.0], [2.0,2.0,2.0], [3.0,3.0,3.0], [4.0,4.0,4.0]]])\n",
    "my_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f1fb2d-fbbb-44e8-9a86-00032a981ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9317, 1.5317, 2.1317, 2.7317],\n",
       "        [1.4714, 2.9714, 4.4714, 5.9714]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_layer = nn.Conv1d(in_channels=3, out_channels=2, kernel_size=1, padding='same')\n",
    "\n",
    "conv1d_layer.weight.data = torch.tensor([[[0.1],  # Filter 1 for Channel 1\n",
    "                                          [0.2],  # Filter 1 for Channel 2\n",
    "                                          [0.3]], # Filter 1 for Channel 3\n",
    "                                        \n",
    "                                         [[0.4],  # Filter 2 for Channel 1\n",
    "                                          [0.5],  # Filter 2 for Channel 2\n",
    "                                          [0.6]], # Filter 2 for Channel 3\n",
    "                                        ])\n",
    "\n",
    "my_output = conv1d_layer(torch.transpose(my_input, 0, 1))\n",
    "my_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7666162-3285-4d18-b606-0d84183aac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9317, 1.4714],\n",
       "        [1.5317, 2.9714],\n",
       "        [2.1317, 4.4714],\n",
       "        [2.7317, 5.9714]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(my_output, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd463df2-4c18-445c-9c80-3fa02b464ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpn_con_layer = ConvLayer(hidden_size=3, kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63b26f8a-c3e3-4e90-a09d-13f3cfb213e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1847, 1.1149, 0.7004],\n",
       "         [0.0458, 3.3364, 2.6179],\n",
       "         [0.9004, 4.6743, 3.4252],\n",
       "         [1.8387, 5.8302, 4.3311]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpn_con_layer(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a14453e4-1318-4a77-bc49-e51769326fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0329, -0.3200,  1.3529],\n",
       "         [-1.2163, -0.0167,  1.2330],\n",
       "         [-1.3117,  0.1980,  1.1137],\n",
       "         [-1.3572,  0.3345,  1.0227]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpn_con_layer.ffn(my_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053845a8-83c4-4724-86c9-08eba7cfb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff0c4b2-e413-4463-bca2-c9485d1e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            TransposeLayer(),\n",
    "            nn.Conv1d(\n",
    "                in_channels=hidden_size,\n",
    "                out_channels=hidden_size,\n",
    "                padding=\"same\",\n",
    "                **kwargs,\n",
    "            ),\n",
    "            TransposeLayer(),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.conv(x)\n",
    "        x = x + self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe4906f-2884-4d8a-b9d0-6370c3543a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fasta\n\u001b[1;32m      3\u001b[0m genome \u001b[38;5;241m=\u001b[39m load_fasta(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/Lan3.1.fna.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m genome\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gpn'"
     ]
    }
   ],
   "source": [
    "from gpn.data import load_fasta\n",
    "\n",
    "genome = load_fasta(\"../data/Lan3.1.fna.gz\")\n",
    "\n",
    "genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff3fdd1-b262-4ccb-98cf-5998226c942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# gpn specific model configuration\n",
    "import gpn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb055af-5a28-4040-93bb-0be92ef6230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer vocabulary: {'[UNK]': 2, 'c': 4, '[MASK]': 1, '[PAD]': 0, 'a': 3, 'g': 5, 't': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvNetForMaskedLM(\n",
       "  (model): ConvNetModel(\n",
       "    (embedding): GPNEmbedding()\n",
       "    (encoder): Sequential(\n",
       "      (0): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(2,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(4,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(8,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(16,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(32,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(2,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(4,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(8,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (10): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(16,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (11): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(32,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (12): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (13): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(2,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (14): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(4,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (15): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(8,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (16): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(16,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (17): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(32,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (18): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (19): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(2,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (20): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(4,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (21): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(8,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (22): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(16,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (23): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same, dilation=(32,))\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (24): ConvLayer(\n",
       "        (conv): Sequential(\n",
       "          (0): TransposeLayer()\n",
       "          (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=same)\n",
       "          (2): TransposeLayer()\n",
       "          (3): GELU(approximate='none')\n",
       "          (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): MLMHead(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=7, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"gonzalobenegas/gpn-brassicales\"\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(f\"tokenizer vocabulary: {tokenizer.get_vocab()}\")\n",
    "\n",
    "# load model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df10f274-aa9f-4713-bfde-04b81f4347ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tacattacatcacaaaaagctaaacacaaagaatcatggattttagattttgatttccacaaaacagagaaatcgaacccgagattgaagaaataactaa'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = genome[\"chr1\"]\n",
    "\n",
    "sequence[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd39047f-69f4-436c-9a66-e9ade5412d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 3, 4, 3, 6, 6, 3, 4, 3, 6, 4, 3, 4, 3, 3, 3, 3, 3, 5, 4, 6, 3, 3, 3,\n",
       "         4, 3, 4, 3, 3, 3, 5, 3, 3, 6, 4, 3, 6, 5, 5, 3, 6, 6, 6, 6, 3, 5, 3, 6,\n",
       "         6, 6, 6, 5, 3, 6, 6, 6, 4, 4, 3, 4, 3, 3, 3, 3, 4, 3, 5, 3, 5, 3, 3, 3,\n",
       "         6, 4, 5, 3, 3, 4, 4, 4, 5, 3, 5, 3, 6, 6, 5, 3, 3, 5, 3, 3, 3, 6, 3, 3,\n",
       "         4, 6, 3, 3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(sequence[:100], return_tensors=\"pt\")['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22470e8-25f7-4baa-9456-261961c43784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acgt_idxs = [tokenizer.get_vocab()[nuc] for nuc in [\"a\", \"c\", \"g\", \"t\"]]\n",
    "acgt_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4e9c10-84ee-40ce-93e9-e4e2012b5354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[-2.3540e+01, -2.3518e+01, -2.3536e+01, -9.0407e-02, -8.0914e-01,\n",
       "          -1.4060e+00,  1.4619e+00],\n",
       "         [-2.4035e+01, -2.4020e+01, -2.4031e+01,  1.5756e+00, -8.3412e-01,\n",
       "          -1.3351e+00, -2.5225e-01],\n",
       "         [-2.2805e+01, -2.2801e+01, -2.2807e+01, -2.6646e-01,  1.3379e+00,\n",
       "          -6.7988e-01, -7.8255e-01],\n",
       "         [-2.5066e+01, -2.5055e+01, -2.5065e+01,  2.3538e+00, -1.3703e+00,\n",
       "          -1.3422e+00, -5.0256e-01],\n",
       "         [-2.1550e+01, -2.1547e+01, -2.1557e+01, -2.0332e-01, -1.0884e+00,\n",
       "          -1.3091e+00,  1.7758e+00],\n",
       "         [-2.2333e+01, -2.2323e+01, -2.2331e+01, -3.9516e-01, -4.7684e-01,\n",
       "          -1.4187e+00,  1.5041e+00],\n",
       "         [-2.4298e+01, -2.4284e+01, -2.4291e+01,  1.7008e+00, -1.1026e+00,\n",
       "          -1.0780e+00, -3.5907e-01],\n",
       "         [-2.1856e+01, -2.1860e+01, -2.1863e+01, -3.1572e-01,  1.6464e+00,\n",
       "          -7.8749e-01, -8.7744e-01],\n",
       "         [-2.5194e+01, -2.5184e+01, -2.5194e+01,  2.4784e+00, -1.0682e+00,\n",
       "          -1.9988e+00, -2.7161e-01],\n",
       "         [-2.1035e+01, -2.1032e+01, -2.1040e+01,  8.5517e-02, -1.1011e+00,\n",
       "          -1.4838e+00,  1.6144e+00],\n",
       "         [-2.2542e+01, -2.2539e+01, -2.2546e+01, -1.1972e-01,  1.3154e+00,\n",
       "          -1.2803e+00, -3.9178e-01],\n",
       "         [-2.3767e+01, -2.3754e+01, -2.3760e+01,  2.0545e+00, -7.4423e-01,\n",
       "          -1.9656e+00, -2.3241e-01],\n",
       "         [-2.2206e+01, -2.2207e+01, -2.2209e+01, -5.8866e-01,  1.5460e+00,\n",
       "          -6.5343e-01, -6.9302e-01],\n",
       "         [-2.3194e+01, -2.3188e+01, -2.3189e+01,  1.8035e+00,  2.7417e-02,\n",
       "          -1.8970e+00, -6.9842e-01],\n",
       "         [-2.2890e+01, -2.2879e+01, -2.2880e+01,  1.5913e+00, -1.5251e-01,\n",
       "          -1.2105e+00, -9.7811e-01],\n",
       "         [-2.2687e+01, -2.2671e+01, -2.2675e+01,  1.5798e+00, -7.6996e-01,\n",
       "          -3.7259e-01, -1.1646e+00],\n",
       "         [-2.2500e+01, -2.2485e+01, -2.2490e+01,  1.6943e+00, -6.6072e-01,\n",
       "          -6.7671e-01, -1.0959e+00],\n",
       "         [-2.3895e+01, -2.3878e+01, -2.3882e+01,  1.9785e+00, -1.0687e+00,\n",
       "          -9.4570e-01, -7.6692e-01],\n",
       "         [-2.0402e+01, -2.0413e+01, -2.0406e+01, -9.5095e-01, -6.8342e-03,\n",
       "           1.1204e+00, -5.3013e-01],\n",
       "         [-2.2660e+01, -2.2664e+01, -2.2661e+01, -3.6653e-01,  1.5001e+00,\n",
       "          -9.1224e-01, -7.1767e-01],\n",
       "         [-2.0836e+01, -2.0834e+01, -2.0848e+01, -9.2960e-02, -6.6367e-01,\n",
       "          -1.6512e+00,  1.4231e+00],\n",
       "         [-2.4215e+01, -2.4198e+01, -2.4210e+01,  1.9262e+00, -5.6488e-01,\n",
       "          -1.3966e+00, -7.3011e-01],\n",
       "         [-2.3420e+01, -2.3406e+01, -2.3411e+01,  2.1280e+00, -6.2521e-01,\n",
       "          -1.6446e+00, -6.1734e-01],\n",
       "         [-2.3320e+01, -2.3303e+01, -2.3309e+01,  2.0975e+00, -1.2160e+00,\n",
       "          -1.2188e+00, -5.2305e-01],\n",
       "         [-2.1885e+01, -2.1888e+01, -2.1890e+01, -2.0671e-01,  1.5567e+00,\n",
       "          -8.5110e-01, -8.3809e-01],\n",
       "         [-2.3595e+01, -2.3585e+01, -2.3590e+01,  1.9335e+00, -2.7274e-01,\n",
       "          -1.9133e+00, -5.7850e-01],\n",
       "         [-2.2489e+01, -2.2494e+01, -2.2496e+01, -4.2121e-01,  1.6300e+00,\n",
       "          -7.9054e-01, -8.0115e-01],\n",
       "         [-2.3406e+01, -2.3395e+01, -2.3395e+01,  1.9669e+00, -6.4423e-01,\n",
       "          -1.4550e+00, -7.1768e-01],\n",
       "         [-2.2823e+01, -2.2810e+01, -2.2814e+01,  1.7259e+00, -6.6829e-01,\n",
       "          -6.9706e-01, -1.0772e+00],\n",
       "         [-2.4143e+01, -2.4126e+01, -2.4133e+01,  1.9909e+00, -9.9042e-01,\n",
       "          -1.1161e+00, -6.7053e-01],\n",
       "         [-2.0409e+01, -2.0428e+01, -2.0424e+01, -4.8858e-01, -2.0118e-01,\n",
       "           1.4040e+00, -9.9740e-01],\n",
       "         [-2.4198e+01, -2.4182e+01, -2.4187e+01,  2.2199e+00, -7.9010e-01,\n",
       "          -9.4024e-01, -1.2580e+00],\n",
       "         [-2.3124e+01, -2.3118e+01, -2.3115e+01,  1.9372e+00, -1.4107e+00,\n",
       "          -1.5582e+00,  1.8052e-01],\n",
       "         [-2.1901e+01, -2.1891e+01, -2.1903e+01,  1.0536e-01, -1.1224e+00,\n",
       "          -1.3713e+00,  1.4649e+00],\n",
       "         [-2.3400e+01, -2.3398e+01, -2.3398e+01, -6.8059e-01,  1.7984e+00,\n",
       "          -1.0581e+00, -4.6719e-01],\n",
       "         [-2.2648e+01, -2.2633e+01, -2.2639e+01,  1.9653e+00, -1.0467e+00,\n",
       "          -1.5734e+00, -2.2341e-01],\n",
       "         [-2.1085e+01, -2.1076e+01, -2.1085e+01, -2.1288e-01, -1.1259e+00,\n",
       "          -1.0355e+00,  1.4749e+00],\n",
       "         [-2.1138e+01, -2.1134e+01, -2.1139e+01, -1.0385e-01, -5.9972e-01,\n",
       "           1.0175e+00, -7.4926e-01],\n",
       "         [-2.2116e+01, -2.2123e+01, -2.2123e+01,  6.6874e-02, -7.5072e-01,\n",
       "           1.5972e+00, -1.2935e+00],\n",
       "         [-2.4017e+01, -2.4009e+01, -2.4010e+01,  1.7942e+00, -1.0423e+00,\n",
       "          -6.1520e-01, -1.0257e+00],\n",
       "         [-2.2656e+01, -2.2648e+01, -2.2659e+01, -4.7042e-01, -1.1070e+00,\n",
       "          -7.6312e-01,  1.3417e+00],\n",
       "         [-2.2222e+01, -2.2210e+01, -2.2220e+01, -8.8707e-01, -3.2817e-01,\n",
       "          -1.0851e+00,  1.2879e+00],\n",
       "         [-2.1874e+01, -2.1865e+01, -2.1872e+01, -6.8021e-01, -3.7483e-02,\n",
       "          -1.1148e+00,  8.2783e-01],\n",
       "         [-2.2041e+01, -2.2030e+01, -2.2044e+01, -9.1568e-01, -1.6217e-01,\n",
       "          -2.2819e-01,  3.5640e-01],\n",
       "         [-2.5295e+01, -2.5284e+01, -2.5291e+01,  1.1156e+00, -6.8584e-02,\n",
       "          -5.3432e-01, -1.4528e+00],\n",
       "         [-2.3779e+01, -2.3785e+01, -2.3784e+01, -4.8997e-01, -3.9411e-01,\n",
       "           1.4656e+00, -1.0995e+00],\n",
       "         [-2.4694e+01, -2.4682e+01, -2.4690e+01,  1.4857e+00, -3.6491e-01,\n",
       "          -6.6097e-01, -1.4365e+00],\n",
       "         [-2.2771e+01, -2.2757e+01, -2.2777e+01, -2.7001e-01, -8.9317e-01,\n",
       "          -8.3911e-01,  9.1918e-01],\n",
       "         [-2.1393e+01, -2.1381e+01, -2.1397e+01, -3.7621e-01, -7.1627e-01,\n",
       "          -1.3456e+00,  1.3960e+00],\n",
       "         [-2.0466e+01, -2.0456e+01, -2.0471e+01, -5.4926e-01, -3.6644e-01,\n",
       "          -1.3321e+00,  1.2169e+00],\n",
       "         [-2.1828e+01, -2.1818e+01, -2.1828e+01, -9.4491e-01,  2.3935e-01,\n",
       "          -1.1000e+00,  7.9805e-01],\n",
       "         [-2.3886e+01, -2.3888e+01, -2.3884e+01, -7.0153e-01, -6.7488e-01,\n",
       "           1.5240e+00, -7.6861e-01],\n",
       "         [-2.3973e+01, -2.3963e+01, -2.3971e+01,  1.6616e+00, -8.7816e-01,\n",
       "          -5.7848e-01, -1.1335e+00],\n",
       "         [-2.2669e+01, -2.2660e+01, -2.2674e+01, -3.7022e-01, -1.2925e+00,\n",
       "          -7.5810e-01,  1.4507e+00],\n",
       "         [-2.2595e+01, -2.2590e+01, -2.2600e+01, -3.9338e-01, -7.8267e-01,\n",
       "          -1.4930e+00,  1.7189e+00],\n",
       "         [-2.1589e+01, -2.1580e+01, -2.1593e+01, -5.3994e-01, -6.1233e-01,\n",
       "          -1.4139e+00,  1.6090e+00],\n",
       "         [-2.2560e+01, -2.2560e+01, -2.2564e+01, -4.3814e-01,  1.5388e+00,\n",
       "          -9.3959e-01, -6.4039e-01],\n",
       "         [-2.1936e+01, -2.1937e+01, -2.1942e+01, -2.3829e-01,  1.3798e+00,\n",
       "          -9.9894e-01, -6.1582e-01],\n",
       "         [-2.3592e+01, -2.3578e+01, -2.3586e+01,  1.8047e+00, -4.5662e-01,\n",
       "          -1.6500e+00, -6.3598e-01],\n",
       "         [-2.1544e+01, -2.1547e+01, -2.1552e+01, -8.8623e-01,  1.5043e+00,\n",
       "          -3.4088e-01, -6.9110e-01],\n",
       "         [-2.3417e+01, -2.3405e+01, -2.3412e+01,  1.8093e+00, -9.5488e-01,\n",
       "          -9.6302e-01, -8.0846e-01],\n",
       "         [-2.3212e+01, -2.3195e+01, -2.3202e+01,  1.9522e+00, -1.1675e+00,\n",
       "          -7.1691e-01, -9.2830e-01],\n",
       "         [-2.5037e+01, -2.5015e+01, -2.5026e+01,  2.1694e+00, -9.7781e-01,\n",
       "          -1.2644e+00, -7.9251e-01],\n",
       "         [-2.3321e+01, -2.3305e+01, -2.3308e+01,  1.7182e+00, -8.8286e-01,\n",
       "          -1.2962e+00, -4.6325e-01],\n",
       "         [-2.2180e+01, -2.2178e+01, -2.2190e+01, -1.1544e+00,  2.1079e+00,\n",
       "          -8.0626e-01, -4.8235e-01],\n",
       "         [-2.3731e+01, -2.3720e+01, -2.3724e+01,  1.7886e+00, -9.7018e-01,\n",
       "          -1.4211e+00, -3.9118e-01],\n",
       "         [-2.1798e+01, -2.1810e+01, -2.1823e+01, -9.2258e-01, -5.8553e-01,\n",
       "           1.7575e+00, -6.5320e-01],\n",
       "         [-2.4688e+01, -2.4668e+01, -2.4676e+01,  2.1306e+00, -1.3012e+00,\n",
       "          -1.4405e+00, -2.6576e-01],\n",
       "         [-2.2651e+01, -2.2661e+01, -2.2672e+01, -8.7540e-01, -8.4655e-01,\n",
       "           2.1187e+00, -7.2709e-01],\n",
       "         [-2.3994e+01, -2.3972e+01, -2.3982e+01,  2.1121e+00, -1.0892e+00,\n",
       "          -7.4837e-01, -1.0788e+00],\n",
       "         [-2.4175e+01, -2.4158e+01, -2.4165e+01,  2.3626e+00, -1.2405e+00,\n",
       "          -8.2364e-01, -1.1067e+00],\n",
       "         [-2.5075e+01, -2.5065e+01, -2.5069e+01,  2.3361e+00, -8.9845e-01,\n",
       "          -1.9038e+00, -3.9366e-01],\n",
       "         [-2.1930e+01, -2.1926e+01, -2.1936e+01, -5.5446e-01, -8.2152e-01,\n",
       "          -1.5411e+00,  2.0466e+00],\n",
       "         [-2.3424e+01, -2.3417e+01, -2.3427e+01, -7.8084e-01,  1.5700e+00,\n",
       "          -9.1084e-01, -2.7445e-01],\n",
       "         [-1.9382e+01, -1.9377e+01, -1.9394e+01,  5.9351e-01, -1.5787e+00,\n",
       "           1.5017e+00, -9.6244e-01],\n",
       "         [-2.4322e+01, -2.4298e+01, -2.4306e+01,  3.1808e+00, -1.5085e+00,\n",
       "          -1.2129e+00, -1.1331e+00],\n",
       "         [-2.3174e+01, -2.3158e+01, -2.3161e+01,  2.3540e+00, -1.6254e+00,\n",
       "          -1.3273e+00, -2.5380e-01],\n",
       "         [-2.0755e+01, -2.0753e+01, -2.0753e+01,  2.0378e-01,  4.6584e-01,\n",
       "          -1.1176e+00,  5.1701e-02],\n",
       "         [-2.2392e+01, -2.2391e+01, -2.2397e+01,  9.4526e-02,  1.1241e+00,\n",
       "          -1.7738e+00,  1.2059e-01],\n",
       "         [-2.1351e+01, -2.1349e+01, -2.1356e+01,  3.5827e-01,  7.4232e-01,\n",
       "          -1.1270e+00, -4.5511e-01],\n",
       "         [-1.9406e+01, -1.9409e+01, -1.9420e+01, -5.7161e-01, -1.0982e+00,\n",
       "           6.2320e-01,  2.3078e-01],\n",
       "         [-2.4525e+01, -2.4510e+01, -2.4519e+01,  2.4907e+00, -1.4864e+00,\n",
       "          -1.2743e+00, -5.4694e-01],\n",
       "         [-2.0387e+01, -2.0383e+01, -2.0395e+01,  8.5692e-01, -1.4639e+00,\n",
       "           1.6428e+00, -1.4314e+00],\n",
       "         [-2.4588e+01, -2.4579e+01, -2.4579e+01,  2.4466e+00, -1.2188e+00,\n",
       "          -1.5553e+00, -5.1903e-01],\n",
       "         [-2.0905e+01, -2.0898e+01, -2.0904e+01, -1.7265e-01, -1.4884e+00,\n",
       "          -1.1713e+00,  1.9833e+00],\n",
       "         [-2.2053e+01, -2.2041e+01, -2.2058e+01, -7.2303e-01, -7.1076e-01,\n",
       "          -1.3323e+00,  1.9602e+00],\n",
       "         [-2.3122e+01, -2.3128e+01, -2.3135e+01, -6.2796e-01, -5.7930e-01,\n",
       "           1.6936e+00, -8.4808e-01],\n",
       "         [-2.2742e+01, -2.2724e+01, -2.2729e+01,  2.2234e+00, -1.0556e+00,\n",
       "          -7.1043e-01, -1.1821e+00],\n",
       "         [-2.4226e+01, -2.4211e+01, -2.4214e+01,  2.1625e+00, -1.6197e+00,\n",
       "          -8.8763e-01, -4.6254e-01],\n",
       "         [-2.2657e+01, -2.2667e+01, -2.2671e+01, -6.4447e-01, -5.5453e-01,\n",
       "           1.9253e+00, -1.0310e+00],\n",
       "         [-2.4479e+01, -2.4462e+01, -2.4468e+01,  2.1765e+00, -5.9265e-01,\n",
       "          -1.1198e+00, -1.2063e+00],\n",
       "         [-2.3193e+01, -2.3173e+01, -2.3175e+01,  1.9506e+00, -1.2002e+00,\n",
       "          -5.9051e-01, -9.1035e-01],\n",
       "         [-2.3374e+01, -2.3361e+01, -2.3369e+01,  1.5919e+00, -1.0025e+00,\n",
       "          -6.7738e-01, -7.2100e-01],\n",
       "         [-2.1025e+01, -2.1014e+01, -2.1025e+01, -3.1281e-01, -7.4958e-01,\n",
       "          -6.3057e-01,  8.6401e-01],\n",
       "         [-2.3611e+01, -2.3595e+01, -2.3602e+01,  1.3962e+00, -3.9797e-01,\n",
       "          -1.0345e+00, -7.2271e-01],\n",
       "         [-2.2674e+01, -2.2663e+01, -2.2666e+01,  1.6502e+00, -8.7948e-01,\n",
       "          -9.6018e-01, -6.7044e-01],\n",
       "         [-2.1977e+01, -2.1981e+01, -2.1980e+01, -4.7858e-01,  1.3237e+00,\n",
       "          -6.5130e-01, -6.2718e-01],\n",
       "         [-2.1198e+01, -2.1189e+01, -2.1202e+01, -2.5533e-01, -6.6553e-01,\n",
       "          -1.0000e+00,  9.9875e-01],\n",
       "         [-2.3082e+01, -2.3067e+01, -2.3076e+01,  1.6060e+00, -8.4114e-01,\n",
       "          -7.9639e-01, -7.7987e-01],\n",
       "         [-2.2518e+01, -2.2506e+01, -2.2511e+01,  1.9791e+00, -1.0714e+00,\n",
       "          -8.5675e-01, -9.4571e-01]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids=input_ids.to(device))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77450ed6-59de-4801-97a8-d16d385432f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2516, 0.2341, 0.2205, 0.2938],\n",
       "         [0.2971, 0.2335, 0.2220, 0.2474],\n",
       "         [0.2449, 0.2875, 0.2350, 0.2326],\n",
       "         [0.3193, 0.2200, 0.2207, 0.2400],\n",
       "         [0.2482, 0.2272, 0.2222, 0.3025],\n",
       "         [0.2437, 0.2417, 0.2200, 0.2946],\n",
       "         [0.3006, 0.2271, 0.2277, 0.2446],\n",
       "         [0.2429, 0.2956, 0.2318, 0.2297],\n",
       "         [0.3226, 0.2263, 0.2062, 0.2450],\n",
       "         [0.2559, 0.2272, 0.2187, 0.2982],\n",
       "         [0.2489, 0.2873, 0.2216, 0.2422],\n",
       "         [0.3105, 0.2347, 0.2077, 0.2470],\n",
       "         [0.2369, 0.2933, 0.2354, 0.2344],\n",
       "         [0.3024, 0.2532, 0.2089, 0.2355],\n",
       "         [0.2968, 0.2493, 0.2243, 0.2296],\n",
       "         [0.2965, 0.2344, 0.2439, 0.2253],\n",
       "         [0.2998, 0.2369, 0.2365, 0.2268],\n",
       "         [0.3083, 0.2273, 0.2301, 0.2343],\n",
       "         [0.2287, 0.2514, 0.2814, 0.2386],\n",
       "         [0.2429, 0.2927, 0.2300, 0.2345],\n",
       "         [0.2523, 0.2383, 0.2159, 0.2936],\n",
       "         [0.3064, 0.2389, 0.2198, 0.2349],\n",
       "         [0.3120, 0.2369, 0.2140, 0.2371],\n",
       "         [0.3120, 0.2240, 0.2239, 0.2401],\n",
       "         [0.2457, 0.2931, 0.2304, 0.2307],\n",
       "         [0.3067, 0.2460, 0.2088, 0.2386],\n",
       "         [0.2407, 0.2955, 0.2320, 0.2317],\n",
       "         [0.3082, 0.2374, 0.2189, 0.2356],\n",
       "         [0.3005, 0.2365, 0.2359, 0.2271],\n",
       "         [0.3085, 0.2290, 0.2261, 0.2364],\n",
       "         [0.2388, 0.2457, 0.2885, 0.2269],\n",
       "         [0.3149, 0.2331, 0.2296, 0.2224],\n",
       "         [0.3068, 0.2195, 0.2163, 0.2574],\n",
       "         [0.2569, 0.2272, 0.2216, 0.2943],\n",
       "         [0.2344, 0.3004, 0.2257, 0.2395],\n",
       "         [0.3081, 0.2280, 0.2163, 0.2476],\n",
       "         [0.2489, 0.2272, 0.2292, 0.2947],\n",
       "         [0.2495, 0.2374, 0.2791, 0.2339],\n",
       "         [0.2525, 0.2327, 0.2943, 0.2204],\n",
       "         [0.3037, 0.2287, 0.2386, 0.2290],\n",
       "         [0.2434, 0.2284, 0.2364, 0.2918],\n",
       "         [0.2336, 0.2470, 0.2290, 0.2904],\n",
       "         [0.2389, 0.2547, 0.2287, 0.2777],\n",
       "         [0.2334, 0.2516, 0.2500, 0.2650],\n",
       "         [0.2849, 0.2531, 0.2416, 0.2204],\n",
       "         [0.2400, 0.2423, 0.2918, 0.2258],\n",
       "         [0.2955, 0.2455, 0.2384, 0.2206],\n",
       "         [0.2493, 0.2343, 0.2355, 0.2808],\n",
       "         [0.2458, 0.2376, 0.2231, 0.2935],\n",
       "         [0.2418, 0.2462, 0.2236, 0.2885],\n",
       "         [0.2325, 0.2618, 0.2289, 0.2768],\n",
       "         [0.2356, 0.2362, 0.2943, 0.2340],\n",
       "         [0.3002, 0.2329, 0.2399, 0.2270],\n",
       "         [0.2455, 0.2239, 0.2361, 0.2945],\n",
       "         [0.2443, 0.2350, 0.2189, 0.3018],\n",
       "         [0.2410, 0.2393, 0.2209, 0.2988],\n",
       "         [0.2410, 0.2937, 0.2292, 0.2362],\n",
       "         [0.2460, 0.2892, 0.2280, 0.2369],\n",
       "         [0.3040, 0.2425, 0.2152, 0.2382],\n",
       "         [0.2301, 0.2922, 0.2430, 0.2346],\n",
       "         [0.3043, 0.2308, 0.2306, 0.2342],\n",
       "         [0.3079, 0.2254, 0.2358, 0.2309],\n",
       "         [0.3142, 0.2293, 0.2229, 0.2336],\n",
       "         [0.3017, 0.2326, 0.2232, 0.2426],\n",
       "         [0.2227, 0.3086, 0.2306, 0.2382],\n",
       "         [0.3041, 0.2308, 0.2206, 0.2445],\n",
       "         [0.2289, 0.2367, 0.2992, 0.2351],\n",
       "         [0.3129, 0.2220, 0.2189, 0.2462],\n",
       "         [0.2290, 0.2297, 0.3089, 0.2324],\n",
       "         [0.3121, 0.2266, 0.2345, 0.2268],\n",
       "         [0.3193, 0.2227, 0.2322, 0.2257],\n",
       "         [0.3186, 0.2305, 0.2085, 0.2425],\n",
       "         [0.2394, 0.2331, 0.2169, 0.3105],\n",
       "         [0.2323, 0.2939, 0.2293, 0.2444],\n",
       "         [0.2663, 0.2143, 0.2916, 0.2279],\n",
       "         [0.3425, 0.2143, 0.2207, 0.2225],\n",
       "         [0.3191, 0.2143, 0.2208, 0.2458],\n",
       "         [0.2572, 0.2641, 0.2254, 0.2533],\n",
       "         [0.2538, 0.2813, 0.2105, 0.2544],\n",
       "         [0.2616, 0.2718, 0.2255, 0.2411],\n",
       "         [0.2404, 0.2281, 0.2709, 0.2605],\n",
       "         [0.3230, 0.2170, 0.2216, 0.2384],\n",
       "         [0.2725, 0.2160, 0.2947, 0.2167],\n",
       "         [0.3219, 0.2231, 0.2157, 0.2393],\n",
       "         [0.2486, 0.2180, 0.2250, 0.3084],\n",
       "         [0.2353, 0.2356, 0.2214, 0.3077],\n",
       "         [0.2356, 0.2368, 0.2972, 0.2305],\n",
       "         [0.3147, 0.2267, 0.2347, 0.2239],\n",
       "         [0.3133, 0.2147, 0.2310, 0.2410],\n",
       "         [0.2345, 0.2366, 0.3032, 0.2256],\n",
       "         [0.3134, 0.2376, 0.2254, 0.2235],\n",
       "         [0.3071, 0.2241, 0.2382, 0.2307],\n",
       "         [0.2974, 0.2295, 0.2371, 0.2360],\n",
       "         [0.2469, 0.2363, 0.2391, 0.2777],\n",
       "         [0.2916, 0.2437, 0.2287, 0.2359],\n",
       "         [0.2994, 0.2325, 0.2306, 0.2374],\n",
       "         [0.2401, 0.2875, 0.2359, 0.2365],\n",
       "         [0.2487, 0.2387, 0.2308, 0.2819],\n",
       "         [0.2979, 0.2332, 0.2343, 0.2346],\n",
       "         [0.3090, 0.2277, 0.2327, 0.2306]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nucleotide_logits = output.logits.cpu().to(torch.float32)[:, :, acgt_idxs]\n",
    "scaled_logits = nucleotide_logits * 0.1\n",
    "output_probs = torch.nn.functional.softmax(scaled_logits, dim=-1)\n",
    "output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b042643-899f-4415-9794-5f2fd3d15870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_generator(\n",
    "    sequence, start_position, stop_position, tokenizer, window_size=513, step_size=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate sliding windows over a DNA sequence.\n",
    "\n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file\n",
    "        window_size (int): Size of the sliding window\n",
    "        step_size (int): Step size for sliding the window\n",
    "\n",
    "    Yields:\n",
    "        dict: A dictionary with the sequence window\n",
    "    \"\"\"\n",
    "    seq_len = len(sequence)\n",
    "\n",
    "    window_size_half = window_size // 2\n",
    "\n",
    "    assert start_position - window_size_half - 1 >= 0\n",
    "    assert stop_position + window_size_half - 1 <= seq_len\n",
    "\n",
    "    for position in range(start_position, stop_position + 1, step_size):\n",
    "        # arrays are 0-indexed, genomes 1-indexed\n",
    "        position = position - 1\n",
    "\n",
    "        start = int(position - window_size_half)\n",
    "        end = int(position + window_size_half + 1)\n",
    "\n",
    "        sequence_window = sequence[start:end]\n",
    "\n",
    "        center = len(sequence_window) // 2\n",
    "\n",
    "        tokenized_input = tokenizer(\n",
    "            sequence_window,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "\n",
    "        # Remove the batch dimension for dataset compatibility\n",
    "        tokenized_data = {k: v.squeeze(0) for k, v in tokenized_input.items()}\n",
    "\n",
    "        # mask the center nucleotide\n",
    "        tokenized_data[\"input_ids\"][center] = tokenizer.mask_token_id\n",
    "        tokenized_data[\"reference\"] = sequence_window[center]\n",
    "\n",
    "        # Add position information\n",
    "        tokenized_data[\"position\"] = position + 1 # back to genome index\n",
    "        tokenized_data[\"sequence\"] = (\n",
    "            sequence_window  # Keep the original sequence for reference\n",
    "        )\n",
    "\n",
    "        yield tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a05baf3e-a5b7-4513-bb8a-180f3f9fe755",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = sliding_window_generator(sequence, 10, 30, tokenizer, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cef453d0-51a1-44e3-88c7-10457440c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([4, 3, 1, 4, 3]),\n",
       " 'reference': 't',\n",
       " 'position': 10,\n",
       " 'sequence': 'catca'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e31f1e8-0f21-43c3-a55e-64131a5d5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_generator(\n",
    "    lambda: sliding_window_generator(\n",
    "        sequence, 3, 98, tokenizer, window_size=5\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5de20a2-74b6-48a2-8069-8ca3a389e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([item[\"input_ids\"] for item in batch]),\n",
    "        \"reference\": [item[\"reference\"] for item in batch],\n",
    "        \"sequence\": [item[\"sequence\"] for item in batch],\n",
    "        \"position\": [item[\"position\"] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f672d3-72e3-48ee-9e25-0aaeafc3ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,  # For sliding windows, keep in order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5ec063-a30a-4b6c-afda-944aa5396184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>reference</th>\n",
       "      <th>p_a</th>\n",
       "      <th>p_c</th>\n",
       "      <th>p_g</th>\n",
       "      <th>p_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>tensor(0.2848)</td>\n",
       "      <td>tensor(0.1555)</td>\n",
       "      <td>tensor(0.1534)</td>\n",
       "      <td>tensor(0.4063)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>tensor(0.3722)</td>\n",
       "      <td>tensor(0.1678)</td>\n",
       "      <td>tensor(0.1348)</td>\n",
       "      <td>tensor(0.3252)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>tensor(0.3382)</td>\n",
       "      <td>tensor(0.1370)</td>\n",
       "      <td>tensor(0.1793)</td>\n",
       "      <td>tensor(0.3454)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>tensor(0.2366)</td>\n",
       "      <td>tensor(0.3201)</td>\n",
       "      <td>tensor(0.1349)</td>\n",
       "      <td>tensor(0.3083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>tensor(0.2398)</td>\n",
       "      <td>tensor(0.1684)</td>\n",
       "      <td>tensor(0.2458)</td>\n",
       "      <td>tensor(0.3459)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>94</td>\n",
       "      <td>t</td>\n",
       "      <td>tensor(0.2969)</td>\n",
       "      <td>tensor(0.2202)</td>\n",
       "      <td>tensor(0.2690)</td>\n",
       "      <td>tensor(0.2139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>95</td>\n",
       "      <td>a</td>\n",
       "      <td>tensor(0.2366)</td>\n",
       "      <td>tensor(0.3201)</td>\n",
       "      <td>tensor(0.1349)</td>\n",
       "      <td>tensor(0.3083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>96</td>\n",
       "      <td>a</td>\n",
       "      <td>tensor(0.2610)</td>\n",
       "      <td>tensor(0.1427)</td>\n",
       "      <td>tensor(0.2131)</td>\n",
       "      <td>tensor(0.3831)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>97</td>\n",
       "      <td>c</td>\n",
       "      <td>tensor(0.3351)</td>\n",
       "      <td>tensor(0.1238)</td>\n",
       "      <td>tensor(0.1086)</td>\n",
       "      <td>tensor(0.4325)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>98</td>\n",
       "      <td>t</td>\n",
       "      <td>tensor(0.4773)</td>\n",
       "      <td>tensor(0.2683)</td>\n",
       "      <td>tensor(0.1052)</td>\n",
       "      <td>tensor(0.1492)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    position reference             p_a             p_c             p_g  \\\n",
       "0          3         c  tensor(0.2848)  tensor(0.1555)  tensor(0.1534)   \n",
       "1          4         a  tensor(0.3722)  tensor(0.1678)  tensor(0.1348)   \n",
       "2          5         t  tensor(0.3382)  tensor(0.1370)  tensor(0.1793)   \n",
       "3          6         t  tensor(0.2366)  tensor(0.3201)  tensor(0.1349)   \n",
       "4          7         a  tensor(0.2398)  tensor(0.1684)  tensor(0.2458)   \n",
       "..       ...       ...             ...             ...             ...   \n",
       "91        94         t  tensor(0.2969)  tensor(0.2202)  tensor(0.2690)   \n",
       "92        95         a  tensor(0.2366)  tensor(0.3201)  tensor(0.1349)   \n",
       "93        96         a  tensor(0.2610)  tensor(0.1427)  tensor(0.2131)   \n",
       "94        97         c  tensor(0.3351)  tensor(0.1238)  tensor(0.1086)   \n",
       "95        98         t  tensor(0.4773)  tensor(0.2683)  tensor(0.1052)   \n",
       "\n",
       "               p_t  \n",
       "0   tensor(0.4063)  \n",
       "1   tensor(0.3252)  \n",
       "2   tensor(0.3454)  \n",
       "3   tensor(0.3083)  \n",
       "4   tensor(0.3459)  \n",
       "..             ...  \n",
       "91  tensor(0.2139)  \n",
       "92  tensor(0.3083)  \n",
       "93  tensor(0.3831)  \n",
       "94  tensor(0.4325)  \n",
       "95  tensor(0.1492)  \n",
       "\n",
       "[96 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions = []\n",
    "acgt_idxs = [tokenizer.get_vocab()[nuc] for nuc in [\"a\", \"c\", \"g\", \"t\"]]\n",
    "\n",
    "window_size = 5\n",
    "center = window_size // 2\n",
    "results = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch)\n",
    "    # batch_results = []\n",
    "    # for i in range(len(batch[\"input_ids\"])):\n",
    "    current_input = batch[\"input_ids\"]\n",
    "    \n",
    "    # print(current_input)\n",
    "    # print(current_input.shape)\n",
    "    with torch.no_grad():\n",
    "        all_logits = model(input_ids=current_input.to(device)).logits.cpu().to(torch.float32)\n",
    "    \n",
    "    nucleotide_logits = all_logits[:, :, acgt_idxs]\n",
    "    output_probs = torch.nn.functional.softmax(nucleotide_logits, dim=-1)\n",
    "    \n",
    "    all_predictions.append(output_probs)\n",
    "\n",
    "    for i in range(len(batch[\"input_ids\"])):\n",
    "        results.append({\n",
    "            \"position\": batch[\"position\"][i],\n",
    "            \"reference\": batch[\"reference\"][i],\n",
    "            \"p_a\": output_probs[i][center][0],\n",
    "            \"p_c\": output_probs[i][center][1],\n",
    "            \"p_g\": output_probs[i][center][2],\n",
    "            \"p_t\": output_probs[i][center][3]\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    # print(batch_results)\n",
    "    # print(model(**batch))\n",
    "    # print(model(input_ids=batch.to(device)).logits.cpu().to(torch.float32))\n",
    "\n",
    "# print(all_predictions)\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5adfe31c-ef5e-4bfa-b9f4-80075efdd5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>reference</th>\n",
       "      <th>p_a</th>\n",
       "      <th>p_c</th>\n",
       "      <th>p_g</th>\n",
       "      <th>p_t</th>\n",
       "      <th>selected</th>\n",
       "      <th>gpn_a</th>\n",
       "      <th>gpn_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>0.284806</td>\n",
       "      <td>0.155489</td>\n",
       "      <td>0.153386</td>\n",
       "      <td>0.406319</td>\n",
       "      <td>0.155489</td>\n",
       "      <td>0.873169</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>0.372197</td>\n",
       "      <td>0.167824</td>\n",
       "      <td>0.134777</td>\n",
       "      <td>0.325202</td>\n",
       "      <td>0.372197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.149117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>0.338208</td>\n",
       "      <td>0.137048</td>\n",
       "      <td>0.179338</td>\n",
       "      <td>0.345406</td>\n",
       "      <td>0.345406</td>\n",
       "      <td>-0.030380</td>\n",
       "      <td>-1.333612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>0.236602</td>\n",
       "      <td>0.320111</td>\n",
       "      <td>0.134939</td>\n",
       "      <td>0.308348</td>\n",
       "      <td>0.308348</td>\n",
       "      <td>-0.382099</td>\n",
       "      <td>0.054014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>0.239819</td>\n",
       "      <td>0.168436</td>\n",
       "      <td>0.245846</td>\n",
       "      <td>0.345899</td>\n",
       "      <td>0.239819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.509739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>94</td>\n",
       "      <td>t</td>\n",
       "      <td>0.296884</td>\n",
       "      <td>0.220174</td>\n",
       "      <td>0.268993</td>\n",
       "      <td>0.213949</td>\n",
       "      <td>0.213949</td>\n",
       "      <td>0.472638</td>\n",
       "      <td>0.041378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>95</td>\n",
       "      <td>a</td>\n",
       "      <td>0.236602</td>\n",
       "      <td>0.320111</td>\n",
       "      <td>0.134939</td>\n",
       "      <td>0.308348</td>\n",
       "      <td>0.236602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>96</td>\n",
       "      <td>a</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.142738</td>\n",
       "      <td>0.213091</td>\n",
       "      <td>0.383137</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.870864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>97</td>\n",
       "      <td>c</td>\n",
       "      <td>0.335135</td>\n",
       "      <td>0.123814</td>\n",
       "      <td>0.108599</td>\n",
       "      <td>0.432451</td>\n",
       "      <td>0.123814</td>\n",
       "      <td>1.436563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>98</td>\n",
       "      <td>t</td>\n",
       "      <td>0.477250</td>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.105211</td>\n",
       "      <td>0.149235</td>\n",
       "      <td>0.149235</td>\n",
       "      <td>1.677165</td>\n",
       "      <td>0.846285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    position reference       p_a       p_c       p_g       p_t  selected  \\\n",
       "0          3         c  0.284806  0.155489  0.153386  0.406319  0.155489   \n",
       "1          4         a  0.372197  0.167824  0.134777  0.325202  0.372197   \n",
       "2          5         t  0.338208  0.137048  0.179338  0.345406  0.345406   \n",
       "3          6         t  0.236602  0.320111  0.134939  0.308348  0.308348   \n",
       "4          7         a  0.239819  0.168436  0.245846  0.345899  0.239819   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "91        94         t  0.296884  0.220174  0.268993  0.213949  0.213949   \n",
       "92        95         a  0.236602  0.320111  0.134939  0.308348  0.236602   \n",
       "93        96         a  0.261034  0.142738  0.213091  0.383137  0.261034   \n",
       "94        97         c  0.335135  0.123814  0.108599  0.432451  0.123814   \n",
       "95        98         t  0.477250  0.268304  0.105211  0.149235  0.149235   \n",
       "\n",
       "       gpn_a     gpn_c  \n",
       "0   0.873169  0.000000  \n",
       "1   0.000000 -1.149117  \n",
       "2  -0.030380 -1.333612  \n",
       "3  -0.382099  0.054014  \n",
       "4   0.000000 -0.509739  \n",
       "..       ...       ...  \n",
       "91  0.472638  0.041378  \n",
       "92  0.000000  0.436113  \n",
       "93  0.000000 -0.870864  \n",
       "94  1.436563  0.000000  \n",
       "95  1.677165  0.846285  \n",
       "\n",
       "[96 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p_reference = results['p_' + results['reference']]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df = results\n",
    "\n",
    "p_reference = [row[col] for row, col in zip(df.to_dict('records'), 'p_' + df['reference'])]\n",
    "\n",
    "# p_reference\n",
    "results['gpn_a'] = results['p_a'] / p_reference\n",
    "results['gpn_c'] = results['p_c'] / p_reference\n",
    "\n",
    "results[['gpn_a', 'gpn_c']] = np.log2(results[['gpn_a', 'gpn_c']])\n",
    "\n",
    "results\n",
    "\n",
    "df = results\n",
    "\n",
    "df = df.map(lambda x: x.item() if torch.is_tensor(x) and x.numel() == 1 else x)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289c1c2-e4f6-446b-8530-8a8e2afb72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotides = [\"a\", \"c\", \"g\", \"t\"]\n",
    "alternatives = [n for n in nucleotides if n != reference_nucleotide]\n",
    "\n",
    "gpn_scores = probabilities_per_context_length\n",
    "\n",
    "for alt in alternatives:\n",
    "    gpn_scores[f\"gpn_{alt}\"] = gpn_scores[alt] / gpn_scores[reference_nucleotide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ad0e4a-a316-45cf-8f61-69978dcdfec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting biopython\n",
      "  Obtaining dependency information for biopython from https://files.pythonhosted.org/packages/25/9c/612821b946930b6caa5d795cfe4169ed6a522562eced9776914be7efaf21/biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting numpy (from biopython)\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/ad/c9/1bf6ada582eebcbe8978f5feb26584cd2b39f94ededeea034ca8f84af8c8/numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, biopython\n",
      "Successfully installed biopython-1.85 numpy-2.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f3618f-da11-43f0-a329-775382233c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqIO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99b801a1-131e-4d5c-b6ec-ade53f44b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fwd::AccID1925|tig00000049:3041-6378 3337\n",
      "fwd::AccID1925|tig00000049:8135-11119 2885\n",
      "fwd::AccID5993|tig00000115:3634-5812 2178\n",
      "fwd::AccID6899|tig00000128:2515-5681 3166\n",
      "fwd::AccID6906|tig00000007:13910-17050 3140\n",
      "fwd::AccID6906|tig00000007:2204-5350 3146\n",
      "fwd::AccID6906|tig00000007:8230-11022 2792\n",
      "fwd::AccID6924|tig00000062:2370-4184 1814\n",
      "fwd::AccID6939|tig00000031:2959-6215 3256\n",
      "fwd::AccID6939|tig00000031:9021-12010 2989\n",
      "fwd::AccID6939|tig00000145:2337-5461 3124\n",
      "fwd::AccID6981|tig00000112:1855-4963 3108\n",
      "fwd::AccID7058|tig00000009:1803-5004 3201\n",
      "fwd::AccID7058|tig00000009:7773-10686 2913\n",
      "fwd::AccID7058|tig00000016:3734-6792 3058\n",
      "fwd::AccID7067|tig00000089:1956-5141 3185\n",
      "fwd::AccID7111|tig00000075:1932-5216 3284\n",
      "fwd::AccID7273|tig00000028:1383-4635 3252\n",
      "fwd::AccID7273|tig00000028:7394-10589 3195\n",
      "fwd::AccID7288|tig00000036:14474-17662 3188\n",
      "fwd::AccID7288|tig00000036:8810-11999 3189\n",
      "fwd::AccID7288|tig00000091:3888-6478 2590\n",
      "fwd::AccID7322|tig00000075:1559-4754 3195\n",
      "fwd::AccID7328|tig00000056:4784-7868 3084\n",
      "fwd::AccID7328|tig00000088:6361-9454 3093\n",
      "fwd::AccID7373|tig00000033:1339-4546 3207\n",
      "fwd::AccID7373|tig00000033:7137-10141 3004\n",
      "fwd::AccID7413|tig00000062:3460-6538 3078\n",
      "fwd::AccID7415|tig00000071:1796-5103 3307\n",
      "fwd::AccID7415|tig00000104:2452-4628 2176\n",
      "fwd::AccID7416|tig00000006:3152-6489 3337\n",
      "fwd::AccID7416|tig00000006:8346-11230 2885\n",
      "fwd::AccID7416|tig00000051:2340-5424 3084\n",
      "fwd::AccID7417|tig00000017:2276-5423 3147\n",
      "fwd::AccID7417|tig00000017:8315-11487 3172\n",
      "fwd::AccID9100|tig00000045:2-3100 3098\n",
      "fwd::AccID9100|tig00000045:5873-9078 3205\n",
      "fwd::AccID9100|tig00000067:2-2994 2992\n",
      "fwd::AccID9100|tig00000068:1583-4871 3288\n",
      "fwd::AccID9134|tig00000034:2727-6036 3309\n",
      "fwd::AccID9134|tig00000034:8818-12005 3187\n",
      "fwd::AccID9332|tig00000044:3166-6408 3242\n",
      "fwd::AccID9332|tig00000044:8885-11982 3097\n",
      "fwd::AccID9332|tig00000096:3215-6312 3097\n",
      "fwd::AccID9518|tig00000024:3039-6365 3326\n",
      "fwd::AccID9518|tig00000024:8741-11898 3157\n",
      "fwd::AccID9518|tig00000094:1767-4879 3112\n",
      "fwd::AccID9533|tig00000028:3839-7154 3315\n",
      "fwd::AccID9533|tig00000028:9631-12827 3196\n",
      "fwd::AccID9533|tig00000094:3188-6297 3109\n",
      "fwd::AccID9536|tig00000021:2476-5790 3314\n",
      "fwd::AccID9536|tig00000021:8028-11230 3202\n",
      "fwd::AccID9536|tig00000083:1827-4908 3081\n",
      "fwd::AccID9537|tig00000080:2758-6055 3297\n",
      "fwd::AccID9543|tig00000008:2899-6216 3317\n",
      "fwd::AccID9543|tig00000008:8451-11664 3213\n",
      "fwd::AccID9549|tig00000159:1424-4625 3201\n",
      "fwd::AccID9549|tig00000159:7453-10016 2563\n",
      "fwd::AccID9550|tig00000068:5736-8728 2992\n",
      "fwd::AccID9550|tig00000110:3493-6645 3152\n",
      "fwd::AccID9557|tig00000032:11369-14161 2792\n",
      "fwd::AccID9557|tig00000032:2720-6009 3289\n",
      "fwd::AccID9557|tig00000059:2425-5104 2679\n",
      "fwd::AccID9583|tig00000052:5160-8106 2946\n",
      "fwd::AccID9600|tig00000091:2529-5612 3083\n",
      "fwd::AccID9655|tig00000039:2399-5595 3196\n",
      "fwd::AccID9655|tig00000039:8191-11225 3034\n",
      "fwd::AccID9655|tig00000084:2237-5533 3296\n",
      "fwd::AccID9669|tig00000093:3345-6451 3106\n",
      "fwd::AccID9721|tig00000018:15530-18452 2922\n",
      "fwd::AccID9721|tig00000018:2317-5485 3168\n",
      "fwd::AccID9721|tig00000018:7854-10643 2789\n",
      "fwd::AccID9721|tig00000047:3352-6046 2694\n",
      "fwd::AccID9721|tig00000047:8587-11556 2969\n",
      "fwd::AccID9784|tig00000041:1258-3640 2382\n",
      "fwd::AccID9784|tig00000041:7440-10438 2998\n",
      "fwd::AccID9784|tig00000071:3693-6888 3195\n",
      "fwd::AccID9792|tig00000077:1466-4773 3307\n",
      "fwd::AccID9792|tig00000093:4445-6621 2176\n",
      "fwd::AccID9837|tig00000069:3333-6530 3197\n",
      "fwd::AccID9837|tig00000190:3914-6453 2539\n",
      "fwd::AccID9869|tig00000087:1956-5037 3081\n",
      "fwd::AccID9871|tig00000120:2-2775 2773\n",
      "fwd::AccID9871|tig00000255:3029-6255 3226\n",
      "fwd::AccID9871|tig00000256:428-3654 3226\n",
      "fwd::AccID9879|tig00000040:3884-7179 3295\n",
      "fwd::AccID9879|tig00000093:2176-5356 3180\n",
      "fwd::AccID9887|tig00000024:2559-5890 3331\n",
      "fwd::AccID9887|tig00000024:8452-11619 3167\n",
      "fwd::AccID9887|tig00000057:2765-5972 3207\n",
      "fwd::AccID9887|tig00000066:3934-7229 3295\n",
      "fwd::AccID9928|tig00000080:3271-6362 3091\n",
      "fwd::AccID9947|tig00000084:2247-5381 3134\n",
      "rev::AccID10015|tig00000062:2866-6057 3191\n",
      "rev::AccID10015|tig00000091:3219-6344 3125\n",
      "rev::AccID108|tig00000028:10461-13612 3151\n",
      "rev::AccID108|tig00000028:4339-7518 3179\n",
      "rev::AccID108|tig00000111:2230-5283 3053\n",
      "rev::AccID1925|tig00000107:4524-7608 3084\n",
      "rev::AccID5784|tig00000053:3328-6480 3152\n",
      "rev::AccID5784|tig00000061:1900-4977 3077\n",
      "rev::AccID5784|tig00000091:3379-6669 3290\n",
      "rev::AccID5993|tig00000028:3421-6629 3208\n",
      "rev::AccID5993|tig00000028:9184-12505 3321\n",
      "rev::AccID6899|tig00000045:2402-4832 2430\n",
      "rev::AccID6899|tig00000045:7537-10678 3141\n",
      "rev::AccID6899|tig00000093:2005-5299 3294\n",
      "rev::AccID6906|tig00000081:2921-5989 3068\n",
      "rev::AccID6909|tig00000028:2373-5580 3207\n",
      "rev::AccID6909|tig00000028:8487-11804 3317\n",
      "rev::AccID6909|tig00000037:2647-5454 2807\n",
      "rev::AccID6909|tig00000037:8245-11277 3032\n",
      "rev::AccID6909|tig00000174:2-2580 2578\n",
      "rev::AccID6924|tig00000014:15529-18182 2653\n",
      "rev::AccID6924|tig00000014:2953-6045 3092\n",
      "rev::AccID6924|tig00000014:8996-12226 3230\n",
      "rev::AccID6974|tig00000052:2265-5342 3077\n",
      "rev::AccID6974|tig00000061:2625-5915 3290\n",
      "rev::AccID6981|tig00000027:10515-13662 3147\n",
      "rev::AccID6981|tig00000027:4490-7704 3214\n",
      "rev::AccID7067|tig00000041:1470-4514 3044\n",
      "rev::AccID7067|tig00000041:6916-9753 2837\n",
      "rev::AccID7111|tig00000042:2323-5233 2910\n",
      "rev::AccID7167|tig00000026:3138-6277 3139\n",
      "rev::AccID7167|tig00000026:8938-12213 3275\n",
      "rev::AccID7167|tig00000113:2049-5065 3016\n",
      "rev::AccID7273|tig00000044:2228-4672 2444\n",
      "rev::AccID7322|tig00000012:2596-5810 3214\n",
      "rev::AccID7322|tig00000012:8621-11500 2879\n",
      "rev::AccID7328|tig00000098:7770-10561 2791\n",
      "rev::AccID7373|tig00000154:1793-4965 3172\n",
      "rev::AccID7373|tig00000156:3399-6609 3210\n",
      "rev::AccID7396|tig00000026:2633-5847 3214\n",
      "rev::AccID7396|tig00000026:8651-11869 3218\n",
      "rev::AccID7396|tig00000048:2052-5062 3010\n",
      "rev::AccID7396|tig00000048:7978-11127 3149\n",
      "rev::AccID7413|tig00000016:16207-18374 2167\n",
      "rev::AccID7413|tig00000016:2051-4973 2922\n",
      "rev::AccID7413|tig00000016:9860-12652 2792\n",
      "rev::AccID7415|tig00000030:1916-4575 2659\n",
      "rev::AccID7415|tig00000048:3839-6903 3064\n",
      "rev::AccID9100|tig00000083:3912-6901 2989\n",
      "rev::AccID9134|tig00000045:3454-6743 3289\n",
      "rev::AccID9518|tig00000047:1847-5012 3165\n",
      "rev::AccID9537|tig00000033:6634-9917 3283\n",
      "rev::AccID9537|tig00000042:6178-9067 2890\n",
      "rev::AccID9537|tig00000042:7720-10407 2687\n",
      "rev::AccID9543|tig00000023:4139-7341 3202\n",
      "rev::AccID9543|tig00000082:2380-5461 3081\n",
      "rev::AccID9550|tig00000037:3772-6938 3166\n",
      "rev::AccID9550|tig00000037:9890-13090 3200\n",
      "rev::AccID9550|tig00000067:5037-7799 2762\n",
      "rev::AccID9554|tig00000010:10072-12874 2802\n",
      "rev::AccID9554|tig00000010:3330-6505 3175\n",
      "rev::AccID9554|tig00000067:3192-6314 3122\n",
      "rev::AccID9580|tig00000027:5331-8525 3194\n",
      "rev::AccID9580|tig00000052:533-3856 3323\n",
      "rev::AccID9580|tig00000075:4944-8044 3100\n",
      "rev::AccID9580|tig00000244:4443-7615 3172\n",
      "rev::AccID9583|tig00000010:4619-7818 3199\n",
      "rev::AccID9583|tig00000036:11487-14810 3323\n",
      "rev::AccID9583|tig00000036:5700-8900 3200\n",
      "rev::AccID9597|tig00000075:1310-4410 3100\n",
      "rev::AccID9597|tig00000075:7194-10406 3212\n",
      "rev::AccID9597|tig00000086:4098-7319 3221\n",
      "rev::AccID9600|tig00000022:10694-13934 3240\n",
      "rev::AccID9600|tig00000022:5376-8163 2787\n",
      "rev::AccID9654|tig00000024:3190-6378 3188\n",
      "rev::AccID9654|tig00000024:9273-12485 3212\n",
      "rev::AccID9654|tig00000054:5293-8458 3165\n",
      "rev::AccID9654|tig00000116:4344-7400 3056\n",
      "rev::AccID9655|tig00000022:3552-6740 3188\n",
      "rev::AccID9655|tig00000022:9635-12844 3209\n",
      "rev::AccID9655|tig00000044:2766-5931 3165\n",
      "rev::AccID9669|tig00000028:3672-6875 3203\n",
      "rev::AccID9669|tig00000028:9633-12885 3252\n",
      "rev::AccID9762|tig00000013:10278-13070 2792\n",
      "rev::AccID9762|tig00000013:15640-18870 3230\n",
      "rev::AccID9762|tig00000013:2785-5707 2922\n",
      "rev::AccID9762|tig00000142:3981-7050 3069\n",
      "rev::AccID9792|tig00000022:4319-6978 2659\n",
      "rev::AccID9792|tig00000022:9951-13168 3217\n",
      "rev::AccID9792|tig00000057:3230-6408 3178\n",
      "rev::AccID9869|tig00000021:3573-6775 3202\n",
      "rev::AccID9869|tig00000021:9012-12326 3314\n",
      "rev::AccID9871|tig00000025:3866-7059 3193\n",
      "rev::AccID9871|tig00000025:9651-12979 3328\n",
      "rev::AccID9879|tig00000010:4090-7019 2929\n",
      "rev::AccID9879|tig00000010:9937-13127 3190\n",
      "rev::AccID9879|tig00000014:3577-6740 3163\n",
      "rev::AccID9928|tig00000041:2068-5276 3208\n",
      "rev::AccID9928|tig00000041:7872-11015 3143\n",
      "rev::AccID9944|tig00000081:2635-5842 3207\n",
      "rev::AccID9944|tig00000092:3395-6592 3197\n",
      "rev::AccID9947|tig00000011:12426-15726 3300\n",
      "rev::AccID9947|tig00000011:6483-9680 3197\n",
      "rev::NW_003302554.1:2626156-2628220 2064\n"
     ]
    }
   ],
   "source": [
    "file_path = \"high_throughput_gpn_computation/resources/B3_long_genes.fasta\"\n",
    "\n",
    "sequence = \"\"\n",
    "\n",
    "from Bio import SeqIO\n",
    "# with gzip.open(file_path, \"rt\") as handle:\n",
    "records = list(SeqIO.parse(file_path, \"fasta\"))\n",
    "\n",
    "names = []\n",
    "\n",
    "for record in records:\n",
    "    print(record.id, len(record.seq))\n",
    "    names.append(record.id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fadcec4-3b03-4ee1-9435-00981291bd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fwd::AccID1925|tig00000049:3041-6378',\n",
       " 'fwd::AccID1925|tig00000049:8135-11119',\n",
       " 'fwd::AccID5993|tig00000115:3634-5812',\n",
       " 'fwd::AccID6899|tig00000128:2515-5681',\n",
       " 'fwd::AccID6906|tig00000007:13910-17050',\n",
       " 'fwd::AccID6906|tig00000007:2204-5350',\n",
       " 'fwd::AccID6906|tig00000007:8230-11022',\n",
       " 'fwd::AccID6924|tig00000062:2370-4184',\n",
       " 'fwd::AccID6939|tig00000031:2959-6215',\n",
       " 'fwd::AccID6939|tig00000031:9021-12010',\n",
       " 'fwd::AccID6939|tig00000145:2337-5461',\n",
       " 'fwd::AccID6981|tig00000112:1855-4963',\n",
       " 'fwd::AccID7058|tig00000009:1803-5004',\n",
       " 'fwd::AccID7058|tig00000009:7773-10686',\n",
       " 'fwd::AccID7058|tig00000016:3734-6792',\n",
       " 'fwd::AccID7067|tig00000089:1956-5141',\n",
       " 'fwd::AccID7111|tig00000075:1932-5216',\n",
       " 'fwd::AccID7273|tig00000028:1383-4635',\n",
       " 'fwd::AccID7273|tig00000028:7394-10589',\n",
       " 'fwd::AccID7288|tig00000036:14474-17662',\n",
       " 'fwd::AccID7288|tig00000036:8810-11999',\n",
       " 'fwd::AccID7288|tig00000091:3888-6478',\n",
       " 'fwd::AccID7322|tig00000075:1559-4754',\n",
       " 'fwd::AccID7328|tig00000056:4784-7868',\n",
       " 'fwd::AccID7328|tig00000088:6361-9454',\n",
       " 'fwd::AccID7373|tig00000033:1339-4546',\n",
       " 'fwd::AccID7373|tig00000033:7137-10141',\n",
       " 'fwd::AccID7413|tig00000062:3460-6538',\n",
       " 'fwd::AccID7415|tig00000071:1796-5103',\n",
       " 'fwd::AccID7415|tig00000104:2452-4628',\n",
       " 'fwd::AccID7416|tig00000006:3152-6489',\n",
       " 'fwd::AccID7416|tig00000006:8346-11230',\n",
       " 'fwd::AccID7416|tig00000051:2340-5424',\n",
       " 'fwd::AccID7417|tig00000017:2276-5423',\n",
       " 'fwd::AccID7417|tig00000017:8315-11487',\n",
       " 'fwd::AccID9100|tig00000045:2-3100',\n",
       " 'fwd::AccID9100|tig00000045:5873-9078',\n",
       " 'fwd::AccID9100|tig00000067:2-2994',\n",
       " 'fwd::AccID9100|tig00000068:1583-4871',\n",
       " 'fwd::AccID9134|tig00000034:2727-6036',\n",
       " 'fwd::AccID9134|tig00000034:8818-12005',\n",
       " 'fwd::AccID9332|tig00000044:3166-6408',\n",
       " 'fwd::AccID9332|tig00000044:8885-11982',\n",
       " 'fwd::AccID9332|tig00000096:3215-6312',\n",
       " 'fwd::AccID9518|tig00000024:3039-6365',\n",
       " 'fwd::AccID9518|tig00000024:8741-11898',\n",
       " 'fwd::AccID9518|tig00000094:1767-4879',\n",
       " 'fwd::AccID9533|tig00000028:3839-7154',\n",
       " 'fwd::AccID9533|tig00000028:9631-12827',\n",
       " 'fwd::AccID9533|tig00000094:3188-6297',\n",
       " 'fwd::AccID9536|tig00000021:2476-5790',\n",
       " 'fwd::AccID9536|tig00000021:8028-11230',\n",
       " 'fwd::AccID9536|tig00000083:1827-4908',\n",
       " 'fwd::AccID9537|tig00000080:2758-6055',\n",
       " 'fwd::AccID9543|tig00000008:2899-6216',\n",
       " 'fwd::AccID9543|tig00000008:8451-11664',\n",
       " 'fwd::AccID9549|tig00000159:1424-4625',\n",
       " 'fwd::AccID9549|tig00000159:7453-10016',\n",
       " 'fwd::AccID9550|tig00000068:5736-8728',\n",
       " 'fwd::AccID9550|tig00000110:3493-6645',\n",
       " 'fwd::AccID9557|tig00000032:11369-14161',\n",
       " 'fwd::AccID9557|tig00000032:2720-6009',\n",
       " 'fwd::AccID9557|tig00000059:2425-5104',\n",
       " 'fwd::AccID9583|tig00000052:5160-8106',\n",
       " 'fwd::AccID9600|tig00000091:2529-5612',\n",
       " 'fwd::AccID9655|tig00000039:2399-5595',\n",
       " 'fwd::AccID9655|tig00000039:8191-11225',\n",
       " 'fwd::AccID9655|tig00000084:2237-5533',\n",
       " 'fwd::AccID9669|tig00000093:3345-6451',\n",
       " 'fwd::AccID9721|tig00000018:15530-18452',\n",
       " 'fwd::AccID9721|tig00000018:2317-5485',\n",
       " 'fwd::AccID9721|tig00000018:7854-10643',\n",
       " 'fwd::AccID9721|tig00000047:3352-6046',\n",
       " 'fwd::AccID9721|tig00000047:8587-11556',\n",
       " 'fwd::AccID9784|tig00000041:1258-3640',\n",
       " 'fwd::AccID9784|tig00000041:7440-10438',\n",
       " 'fwd::AccID9784|tig00000071:3693-6888',\n",
       " 'fwd::AccID9792|tig00000077:1466-4773',\n",
       " 'fwd::AccID9792|tig00000093:4445-6621',\n",
       " 'fwd::AccID9837|tig00000069:3333-6530',\n",
       " 'fwd::AccID9837|tig00000190:3914-6453',\n",
       " 'fwd::AccID9869|tig00000087:1956-5037',\n",
       " 'fwd::AccID9871|tig00000120:2-2775',\n",
       " 'fwd::AccID9871|tig00000255:3029-6255',\n",
       " 'fwd::AccID9871|tig00000256:428-3654',\n",
       " 'fwd::AccID9879|tig00000040:3884-7179',\n",
       " 'fwd::AccID9879|tig00000093:2176-5356',\n",
       " 'fwd::AccID9887|tig00000024:2559-5890',\n",
       " 'fwd::AccID9887|tig00000024:8452-11619',\n",
       " 'fwd::AccID9887|tig00000057:2765-5972',\n",
       " 'fwd::AccID9887|tig00000066:3934-7229',\n",
       " 'fwd::AccID9928|tig00000080:3271-6362',\n",
       " 'fwd::AccID9947|tig00000084:2247-5381',\n",
       " 'rev::AccID10015|tig00000062:2866-6057',\n",
       " 'rev::AccID10015|tig00000091:3219-6344',\n",
       " 'rev::AccID108|tig00000028:10461-13612',\n",
       " 'rev::AccID108|tig00000028:4339-7518',\n",
       " 'rev::AccID108|tig00000111:2230-5283',\n",
       " 'rev::AccID1925|tig00000107:4524-7608',\n",
       " 'rev::AccID5784|tig00000053:3328-6480',\n",
       " 'rev::AccID5784|tig00000061:1900-4977',\n",
       " 'rev::AccID5784|tig00000091:3379-6669',\n",
       " 'rev::AccID5993|tig00000028:3421-6629',\n",
       " 'rev::AccID5993|tig00000028:9184-12505',\n",
       " 'rev::AccID6899|tig00000045:2402-4832',\n",
       " 'rev::AccID6899|tig00000045:7537-10678',\n",
       " 'rev::AccID6899|tig00000093:2005-5299',\n",
       " 'rev::AccID6906|tig00000081:2921-5989',\n",
       " 'rev::AccID6909|tig00000028:2373-5580',\n",
       " 'rev::AccID6909|tig00000028:8487-11804',\n",
       " 'rev::AccID6909|tig00000037:2647-5454',\n",
       " 'rev::AccID6909|tig00000037:8245-11277',\n",
       " 'rev::AccID6909|tig00000174:2-2580',\n",
       " 'rev::AccID6924|tig00000014:15529-18182',\n",
       " 'rev::AccID6924|tig00000014:2953-6045',\n",
       " 'rev::AccID6924|tig00000014:8996-12226',\n",
       " 'rev::AccID6974|tig00000052:2265-5342',\n",
       " 'rev::AccID6974|tig00000061:2625-5915',\n",
       " 'rev::AccID6981|tig00000027:10515-13662',\n",
       " 'rev::AccID6981|tig00000027:4490-7704',\n",
       " 'rev::AccID7067|tig00000041:1470-4514',\n",
       " 'rev::AccID7067|tig00000041:6916-9753',\n",
       " 'rev::AccID7111|tig00000042:2323-5233',\n",
       " 'rev::AccID7167|tig00000026:3138-6277',\n",
       " 'rev::AccID7167|tig00000026:8938-12213',\n",
       " 'rev::AccID7167|tig00000113:2049-5065',\n",
       " 'rev::AccID7273|tig00000044:2228-4672',\n",
       " 'rev::AccID7322|tig00000012:2596-5810',\n",
       " 'rev::AccID7322|tig00000012:8621-11500',\n",
       " 'rev::AccID7328|tig00000098:7770-10561',\n",
       " 'rev::AccID7373|tig00000154:1793-4965',\n",
       " 'rev::AccID7373|tig00000156:3399-6609',\n",
       " 'rev::AccID7396|tig00000026:2633-5847',\n",
       " 'rev::AccID7396|tig00000026:8651-11869',\n",
       " 'rev::AccID7396|tig00000048:2052-5062',\n",
       " 'rev::AccID7396|tig00000048:7978-11127',\n",
       " 'rev::AccID7413|tig00000016:16207-18374',\n",
       " 'rev::AccID7413|tig00000016:2051-4973',\n",
       " 'rev::AccID7413|tig00000016:9860-12652',\n",
       " 'rev::AccID7415|tig00000030:1916-4575',\n",
       " 'rev::AccID7415|tig00000048:3839-6903',\n",
       " 'rev::AccID9100|tig00000083:3912-6901',\n",
       " 'rev::AccID9134|tig00000045:3454-6743',\n",
       " 'rev::AccID9518|tig00000047:1847-5012',\n",
       " 'rev::AccID9537|tig00000033:6634-9917',\n",
       " 'rev::AccID9537|tig00000042:6178-9067',\n",
       " 'rev::AccID9537|tig00000042:7720-10407',\n",
       " 'rev::AccID9543|tig00000023:4139-7341',\n",
       " 'rev::AccID9543|tig00000082:2380-5461',\n",
       " 'rev::AccID9550|tig00000037:3772-6938',\n",
       " 'rev::AccID9550|tig00000037:9890-13090',\n",
       " 'rev::AccID9550|tig00000067:5037-7799',\n",
       " 'rev::AccID9554|tig00000010:10072-12874',\n",
       " 'rev::AccID9554|tig00000010:3330-6505',\n",
       " 'rev::AccID9554|tig00000067:3192-6314',\n",
       " 'rev::AccID9580|tig00000027:5331-8525',\n",
       " 'rev::AccID9580|tig00000052:533-3856',\n",
       " 'rev::AccID9580|tig00000075:4944-8044',\n",
       " 'rev::AccID9580|tig00000244:4443-7615',\n",
       " 'rev::AccID9583|tig00000010:4619-7818',\n",
       " 'rev::AccID9583|tig00000036:11487-14810',\n",
       " 'rev::AccID9583|tig00000036:5700-8900',\n",
       " 'rev::AccID9597|tig00000075:1310-4410',\n",
       " 'rev::AccID9597|tig00000075:7194-10406',\n",
       " 'rev::AccID9597|tig00000086:4098-7319',\n",
       " 'rev::AccID9600|tig00000022:10694-13934',\n",
       " 'rev::AccID9600|tig00000022:5376-8163',\n",
       " 'rev::AccID9654|tig00000024:3190-6378',\n",
       " 'rev::AccID9654|tig00000024:9273-12485',\n",
       " 'rev::AccID9654|tig00000054:5293-8458',\n",
       " 'rev::AccID9654|tig00000116:4344-7400',\n",
       " 'rev::AccID9655|tig00000022:3552-6740',\n",
       " 'rev::AccID9655|tig00000022:9635-12844',\n",
       " 'rev::AccID9655|tig00000044:2766-5931',\n",
       " 'rev::AccID9669|tig00000028:3672-6875',\n",
       " 'rev::AccID9669|tig00000028:9633-12885',\n",
       " 'rev::AccID9762|tig00000013:10278-13070',\n",
       " 'rev::AccID9762|tig00000013:15640-18870',\n",
       " 'rev::AccID9762|tig00000013:2785-5707',\n",
       " 'rev::AccID9762|tig00000142:3981-7050',\n",
       " 'rev::AccID9792|tig00000022:4319-6978',\n",
       " 'rev::AccID9792|tig00000022:9951-13168',\n",
       " 'rev::AccID9792|tig00000057:3230-6408',\n",
       " 'rev::AccID9869|tig00000021:3573-6775',\n",
       " 'rev::AccID9869|tig00000021:9012-12326',\n",
       " 'rev::AccID9871|tig00000025:3866-7059',\n",
       " 'rev::AccID9871|tig00000025:9651-12979',\n",
       " 'rev::AccID9879|tig00000010:4090-7019',\n",
       " 'rev::AccID9879|tig00000010:9937-13127',\n",
       " 'rev::AccID9879|tig00000014:3577-6740',\n",
       " 'rev::AccID9928|tig00000041:2068-5276',\n",
       " 'rev::AccID9928|tig00000041:7872-11015',\n",
       " 'rev::AccID9944|tig00000081:2635-5842',\n",
       " 'rev::AccID9944|tig00000092:3395-6592',\n",
       " 'rev::AccID9947|tig00000011:12426-15726',\n",
       " 'rev::AccID9947|tig00000011:6483-9680',\n",
       " 'rev::NW_003302554.1:2626156-2628220']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f81e1a6-8162-43ba-b75e-b53188ecd097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197530"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.find(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9023cc9f-7c73-40e8-8ba1-5927500c31b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/cd/5f/4dba1d39bb9c38d574a9a22548c540177f78ea47b32f99c0ff2ec499fac5/pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /home/sbuedenb/.local/lib/python3.11/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /projects/sw/eb/arch/zen4/software/Miniconda3/23.9.0-0/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /projects/sw/eb/arch/zen4/software/Miniconda3/23.9.0-0/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /projects/sw/eb/arch/zen4/software/Miniconda3/23.9.0-0/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691e08b8-0ff4-409d-a68b-134c314970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8757e4-452f-49fd-8974-a8619a8d4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19189381-be00-4cea-b412-73fcf67b24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b981f7b-eaf0-467b-bcce-22a77259e517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "2  2\n",
       "1  1\n",
       "0  0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7779a178-9bee-4118-9c8c-5b099f3797dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24c59e4a-9411-442e-9e7d-d1108fc1a207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[len(b)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e6fadc8-409b-4013-9a25-7d71f8234478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 9, 27, 81, 243, 729, 2187, 6561, 19683, 59049, 177147]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3**i for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0d6adf3-01ea-41bf-8839-764113bb3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_contig(arr):\n",
    "    def update(state, curr):\n",
    "        last, pos, acc = state\n",
    "        if last == None:\n",
    "            acc.append(1)\n",
    "            return (curr, pos, acc)\n",
    "        if curr == last:\n",
    "            acc[pos] += 1\n",
    "            return (curr, pos, acc)\n",
    "        else:\n",
    "            acc.append(1)\n",
    "            return (curr, pos+1, acc)\n",
    "        \n",
    "    _, _, result = reduce(update, arr, (None, 0, []))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b93b1ae-90c9-4630-98b0-90d508e1c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 1, 2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_contig([1,1, 2,3 , 2, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c28ee17-8e01-46c7-84e1-d6e14c24c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plot\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03365935-7dc1-4aed-baf0-6cb49991fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(layer, kernel_size, dilation_base):\n",
    "    dilation = dilation_base**layer\n",
    "    kernel = np.zeros(1 + dilation * (kernel_size - 1))\n",
    "    for i in range(len(kernel)):\n",
    "        if i % dilation == 0:\n",
    "            kernel[i] = 1\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c1acf2-f8a0-4667-9def-db251048421d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel(0,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d91ee689-7b01-4753-845c-e80a381334e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(2)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "893132ab-2527-4660-b831-2045c76c6bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(1)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd33531f-c83c-4127-ace4-6ef3e9a3f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "[3, 3, 21, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(2)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d61edb5e-cd42-43b8-a035-965eeda6d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 57, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 2., 2., 2., 3., 3., 3., 4., 4., 4., 5., 5., 5., 6., 6.,\n",
       "       6., 7., 7., 7., 8., 8., 8., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "       9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "       9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "       9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 8., 8., 8., 7.,\n",
       "       7., 7., 6., 6., 6., 5., 5., 5., 4., 4., 4., 3., 3., 3., 2., 2., 2.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(3)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eda1a05f-ec0c-4e75-863c-7879356cafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 165, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.,  5.,\n",
       "        5.,  5.,  6.,  6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.,  9.,  9.,\n",
       "        9., 10., 10., 10., 11., 11., 11., 12., 12., 12., 13., 13., 13.,\n",
       "       14., 14., 14., 15., 15., 15., 16., 16., 16., 17., 17., 17., 18.,\n",
       "       18., 18., 19., 19., 19., 20., 20., 20., 21., 21., 21., 22., 22.,\n",
       "       22., 23., 23., 23., 24., 24., 24., 25., 25., 25., 26., 26., 26.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27., 27.,\n",
       "       27., 27., 27., 27., 27., 27., 27., 27., 27., 26., 26., 26., 25.,\n",
       "       25., 25., 24., 24., 24., 23., 23., 23., 22., 22., 22., 21., 21.,\n",
       "       21., 20., 20., 20., 19., 19., 19., 18., 18., 18., 17., 17., 17.,\n",
       "       16., 16., 16., 15., 15., 15., 14., 14., 14., 13., 13., 13., 12.,\n",
       "       12., 12., 11., 11., 11., 10., 10., 10.,  9.,  9.,  9.,  8.,  8.,\n",
       "        8.,  7.,  7.,  7.,  6.,  6.,  6.,  5.,  5.,  5.,  4.,  4.,  4.,\n",
       "        3.,  3.,  3.,  2.,  2.,  2.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(4)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdd86464-a9cf-470e-8cda-1500efbb9624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 489, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  2.,  2.,  2.,  3.,  3.,  3.,  4.,  4.,  4.,  5.,\n",
       "        5.,  5.,  6.,  6.,  6.,  7.,  7.,  7.,  8.,  8.,  8.,  9.,  9.,\n",
       "        9., 10., 10., 10., 11., 11., 11., 12., 12., 12., 13., 13., 13.,\n",
       "       14., 14., 14., 15., 15., 15., 16., 16., 16., 17., 17., 17., 18.,\n",
       "       18., 18., 19., 19., 19., 20., 20., 20., 21., 21., 21., 22., 22.,\n",
       "       22., 23., 23., 23., 24., 24., 24., 25., 25., 25., 26., 26., 26.,\n",
       "       27., 27., 27., 28., 28., 28., 29., 29., 29., 30., 30., 30., 31.,\n",
       "       31., 31., 32., 32., 32., 33., 33., 33., 34., 34., 34., 35., 35.,\n",
       "       35., 36., 36., 36., 37., 37., 37., 38., 38., 38., 39., 39., 39.,\n",
       "       40., 40., 40., 41., 41., 41., 42., 42., 42., 43., 43., 43., 44.,\n",
       "       44., 44., 45., 45., 45., 46., 46., 46., 47., 47., 47., 48., 48.,\n",
       "       48., 49., 49., 49., 50., 50., 50., 51., 51., 51., 52., 52., 52.,\n",
       "       53., 53., 53., 54., 54., 54., 55., 55., 55., 56., 56., 56., 57.,\n",
       "       57., 57., 58., 58., 58., 59., 59., 59., 60., 60., 60., 61., 61.,\n",
       "       61., 62., 62., 62., 63., 63., 63., 64., 64., 64., 65., 65., 65.,\n",
       "       66., 66., 66., 67., 67., 67., 68., 68., 68., 69., 69., 69., 70.,\n",
       "       70., 70., 71., 71., 71., 72., 72., 72., 73., 73., 73., 74., 74.,\n",
       "       74., 75., 75., 75., 76., 76., 76., 77., 77., 77., 78., 78., 78.,\n",
       "       79., 79., 79., 80., 80., 80., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81., 81.,\n",
       "       81., 80., 80., 80., 79., 79., 79., 78., 78., 78., 77., 77., 77.,\n",
       "       76., 76., 76., 75., 75., 75., 74., 74., 74., 73., 73., 73., 72.,\n",
       "       72., 72., 71., 71., 71., 70., 70., 70., 69., 69., 69., 68., 68.,\n",
       "       68., 67., 67., 67., 66., 66., 66., 65., 65., 65., 64., 64., 64.,\n",
       "       63., 63., 63., 62., 62., 62., 61., 61., 61., 60., 60., 60., 59.,\n",
       "       59., 59., 58., 58., 58., 57., 57., 57., 56., 56., 56., 55., 55.,\n",
       "       55., 54., 54., 54., 53., 53., 53., 52., 52., 52., 51., 51., 51.,\n",
       "       50., 50., 50., 49., 49., 49., 48., 48., 48., 47., 47., 47., 46.,\n",
       "       46., 46., 45., 45., 45., 44., 44., 44., 43., 43., 43., 42., 42.,\n",
       "       42., 41., 41., 41., 40., 40., 40., 39., 39., 39., 38., 38., 38.,\n",
       "       37., 37., 37., 36., 36., 36., 35., 35., 35., 34., 34., 34., 33.,\n",
       "       33., 33., 32., 32., 32., 31., 31., 31., 30., 30., 30., 29., 29.,\n",
       "       29., 28., 28., 28., 27., 27., 27., 26., 26., 26., 25., 25., 25.,\n",
       "       24., 24., 24., 23., 23., 23., 22., 22., 22., 21., 21., 21., 20.,\n",
       "       20., 20., 19., 19., 19., 18., 18., 18., 17., 17., 17., 16., 16.,\n",
       "       16., 15., 15., 15., 14., 14., 14., 13., 13., 13., 12., 12., 12.,\n",
       "       11., 11., 11., 10., 10., 10.,  9.,  9.,  9.,  8.,  8.,  8.,  7.,\n",
       "        7.,  7.,  6.,  6.,  6.,  5.,  5.,  5.,  4.,  4.,  4.,  3.,  3.,\n",
       "        3.,  2.,  2.,  2.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(5)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f55e8507-1e33-4b36-846f-a8f0c3847f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2913\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1461, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "peripheral: 0.4984552008238929\n",
      "center: 0.5015447991761071\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(6)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10a92f55-14a2-4f29-90e8-ffba43394e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8745\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4377, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "peripheral: 0.4994854202401372\n",
      "center: 0.5005145797598628\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(7)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b7b286d-653a-48f8-ad49-543b457b664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26241\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 13125, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "peripheral: 0.49982851263290273\n",
      "center: 0.5001714873670973\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(8)])\n",
    "print(len(rf))\n",
    "print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ed2c513-19c0-49a4-b0cd-eb72129d1c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78729\n",
      "peripheral: 0.4999428419006973\n",
      "center: 0.5000571580993026\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(9)])\n",
    "print(len(rf))\n",
    "# print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fdaf3cb-30fb-4b5d-9df8-893ed023da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236193\n",
      "peripheral: 0.4999809477842273\n",
      "center: 0.5000190522157727\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(10)])\n",
    "print(len(rf))\n",
    "# print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e14bb386-9426-44f6-a95a-3bb084d3902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708585\n",
      "peripheral: 0.4999936493151845\n",
      "center: 0.5000063506848155\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(11)])\n",
    "print(len(rf))\n",
    "# print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "780b00ec-fe58-4372-9221-c3ee7b9af29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125761\n",
      "peripheral: 0.4999978831110365\n",
      "center: 0.5000021168889636\n"
     ]
    }
   ],
   "source": [
    "k = 9\n",
    "b = 3\n",
    "rf = reduce(np.convolve, [kernel(i, k, b) for i in range(12)])\n",
    "print(len(rf))\n",
    "# print(count_contig(rf))\n",
    "[f, _, e] = count_contig(count_contig(rf))\n",
    "print(\"peripheral: \" + str((f + e) * 3 / len(rf)))\n",
    "print(\"center: \" + str(max(count_contig(rf)) / len(rf)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
